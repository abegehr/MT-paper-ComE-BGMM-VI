\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage[numbers]{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[warn]{textcomp}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[super]{nth}
\usepackage{url}
\usepackage{tabularx}

\def\UrlBreaks{\do\/\do-}

% tikz
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
% tikz styles
\tikzstyle{user} = [circle, thick, draw, minimum size=2.5cm]
\tikzstyle{item} = [rectangle, thick, draw, minimum width=3cm, minimum height=1.5cm]
\tikzstyle{arrow} = [->, draw, thick]
\tikzstyle{bi-arrow} = [<->, draw, thick]
\tikzstyle{line} = [-, draw, thick]

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\emergencystretch=8em

\begin{document}

\title{Community Embeddings with Bayesian Gaussian Mixture Model and Variational Inference\\
    \thanks{The reported study was partially supported by RFBR grant \textnumero 20-07-00958. The paper was prepared within the framework of the HSE University Project Group Competition 2020-2022.}
}

\author{
    \IEEEauthorblockN{Anton I. N. Begehr}
    \IEEEauthorblockA{
        \textit{Graduate School of Business}\\
        \textit{National Research University Higher School of Economics}\\
        Moscow, Russia\\
        a.begehr@fu-berlin.de\\
    }
    \and
    \IEEEauthorblockN{Prof. Dr. Petr Panfilov}
    \IEEEauthorblockA{
        \textit{Graduate School of Business}\\
        \textit{National Research University Higher School of Economics}\\
        Moscow, Russia\\
        ppanfilov@hse.ru\\
    }
}

\maketitle

\begin{abstract}
    Graphs, such as social networks, emerge naturally from various real-world situations. Recently, graph embedding methods have gained traction in data science research.
    The graph and community embedding algorithm ComE aims to preserve first-, second- and higher-order proximity. ComE requires prior knowledge of the number of communities K. In this paper, ComE is extended to utilize a Bayesian Gaussian mixture model with variational inference for learning community embeddings (ComE BGMM+VI), similar to ComE+. ComE BGMM+VI takes K as the maximum number of communities and drops components through the trade-off hyperparameter weight concentration prior.
    The advantage of ComE BGMM+VI over the non-Bayesian ComE for an unknown number of communities K is shown for the small Karate club dataset and explored for the larger DBLP dataset.
\end{abstract}

\begin{IEEEkeywords}
    graph, embedding, community embedding, ComE, Bayesian, variational inference, Gaussian mixture, expectation maximization
\end{IEEEkeywords}


\section{Introduction}

Graphs, such as social networks, knowledge graphs, content-rating graphs, and communication networks, emerge naturally from various real-world situations. Analyzing these graphs leads to findings and understanding of the underlying structures, coherences, and dependencies. Recently, methods for embedding graph's nodes into lower-dimensional Euclidean spaces, called graph embeddings, have gained traction in multiple areas of data science research \cite{Goyal_2018}.

Community Embeddings, in addition to embedding a graph's nodes through first- and second-order proximity, also preserve higher-order proximity by embedding clusters present in the graph data. The graph and community embedding algorithm ComE aims to preserve first-, second- and higher-order proximity by embedding a graph's nodes and communities\cite{ComE}. ComE requires prior knowledge of the number of communities $K$. In this paper, ComE is extended to utilizing a Bayesian Gaussian mixture model with variational inference for learning community embeddings (ComE BGMM+VI), similar to ComE+ published by \citeauthor{ComE+} in \citeyear{ComE+} \cite{ComE+}. ComE BGMM+VI takes $K$ as the maximum number of communities and drops components through a trade-off hyperparameter.

The recent \citeyear{ComE} graph embeddings algorithm ComE is extended similarily to the \citeyear{ComE+} ComE+ by taking a Bayesian approach. The open-source code for the Bayesian approach to ComE's community embedding is published on GitHub\footnote{at \url{https://github.com/abegehr/ComE_BGMM}} and serves as a contribution to community embedding research \cite{ComE_BGMM_GH}.

The original ComE paper \textit{Learning Community Embedding with Community Detection and Node Embedding on Graphs} by \citeauthor{ComE} and the ComE+ paper \textit{Embedding Both Finite and Infinite Communities on Graphs} by \citeauthor{ComE+} in combination with the ComE source code have provided the basis and architecture for the community embeddings utilized in this work \cite{ComE, ComE+, ComE_GH}.

Multiple surveys and articles on graph embeddings were consulted to build a full picture of the current state of graph embedding research. Especially the \citeyear{Goyal_2018} survey \textit{Graph Embedding Techniques, Applications, and Performance: A Survey} by \citeauthor{Goyal_2018} and the \citeyear{rossi20tkdd-roles} paper \textit{On Proximity and Structural Role-based Embeddings in Networks: Misconceptions, Techniques, and Applications} by \citeauthor{rossi20tkdd-roles} have proven to be primary resources for understanding the current landscape of graph embedding research \cite{Goyal_2018, rossi20tkdd-roles}.

On part of comparing the Bayesian Gaussian mixture model with variational inference to the Gaussian mixture model with expectation-maximization, the \citeyear{Bishop06} book \textit{Pattern Recognition and Machine Learning (Information Science and Statistics)} by \citeauthor{Bishop06} includes essential statistics and information science knowledge and explanations \cite{Bishop06}.

\section{Graph Embedding}

A graph embedding is a representation of a graph data structure in lower-dimensional space. Utilizing graph embeddings has recently gained traction in the research community for representing and analyzing graph data \cite{Goyal_2018}. Possible applications of graph embeddings include node and graph classification, anomaly detection, link prediction, recommender systems, graph compression, and visualizations \cite{rossi20tkdd-roles}.

There are multiple advantages of graph embeddings over the original graph data-structure $G = (V, E)$:

\begin{enumerate}
    \item \textbf{Machine learning algorithms on graphs are limited \cite{Godec2018}}.Only specific mathematics, statistics, and machine learning algorithms can be applied to the specific graph data-structure $G$ consisting of nodes $V$ and edges $E$.
    \item \textbf{Vector operations are simpler and faster than comparable graph operations \cite{Godec2018}.} A graph embedding is a representation of a graph in lower-dimensional space of dimension $d$, therefore nodes are assigned a feature vector of size $d$, which can then be operated on using vector operations.
    \item \textbf{Embeddings are compressed representations \cite{Godec2018}.} A trivial feature vector for a node $v \in V$ of the graph $G = (V, E)$ is of length $\abs{V}$ with an entry for each node $v_i \in V$ valued by the existing or non-existing, binary or weighted edge. When determining this trivial representation for all nodes $v \in V$, the adjacency matrix of size $\abs{V} \times \abs{V}$ is obtained. An embedding of the same graph $G = (V, E)$ is obtained through embedding $G$ into a $d$-dimensional space. The resulting embedding is of size $\abs{V} \times d$. An embedding is performed into a lower-dimensional space, therefore $d \ll \abs{V}$.
\end{enumerate}

Popular graph embedding algorithms include DeepWalk, introduced by \citeauthor{Perozzi2014DeepWalkOL} in their \citeyear{Perozzi2014DeepWalkOL} paper \textit{DeepWalk}, and Node2Vec, introduced by \citeauthor{Grover2016node2vecSF} in their \citeyear{Grover2016node2vecSF} paper \textit{node2vec: Scalable Feature Learning for Networks} \cite{Perozzi2014DeepWalkOL, Grover2016node2vecSF}. Both DeepWalk and Node2Vec demonstrate competitive results for the use-case of multi-label classification and are promising options for graph embeddings.

\section{Community Embedding}

Recently, a graph embedding algorithm was altered and extended to include the concept of communities. Communities refer to groups of nodes that have high inner connectivity and low outer connectivity to other communities \cite{ComE}. The proximity of nodes hereby is extended to include the concept of higher-order proximity, in addition to first-order and second-order-proximity \cite{ComE}.

\citeauthor{ComE} developed the Community Embedding algorithm ComE in their \citeyear{ComE} paper \textit{Learning Community Embedding with Community Detection and Node Embedding on Graphs} \cite{ComE}.

\subsection{ComE Algorithm}
\label{sec:ComE_alg}

ComE consists of three parts: node embedding, community detection, and community embedding. A closed-loop relationship between node embedding, community detection, and community embedding is indicated by \citeauthor{ComE}. The closed-loop is exploited in an iterative algorithm optimizing the node and community embeddings.

The ComE algorithm is sketched out as follows:

\begin{enumerate}
    \item Sample the graph by executing random walks.
    \item Initialize the node embedding $\Phi$ and context embedding $\Phi'$ with the node embedding algorithm DeepWalk\cite{Perozzi2014DeepWalkOL} with random walks.
    \item Fit a Gaussian mixture model (GMM) representing communities to $(\Phi, \Phi')$ using the expectation-maximization (EM) algorithm, while keeping $(\Phi, \Phi')$ fixed. Yield the parameters of the GMM as the community embedding: mixed community membership $\Pi$, Gaussian means $\Psi$, and Gaussian covariances $\Sigma$.
    \item Use stochastic decent with first-, second-, and higher-order proximity optimization functions to adjust $(\Phi, \Phi')$ to the Gaussian mixture model, keeping the GMM's parameters $(\Pi, \Psi, \Sigma)$ fixed. Yield the next node embedding and context embedding $(\Phi, \Phi')$ updated by first-, second-, and higher-order proximity.
    \item Repeat steps 3 and 4 at will. Step 3 represents community detection and embedding. Step 4 represents updating the node embedding.
\end{enumerate}

The formulas and concepts used in this chapter are borrowed from the original ComE paper.\cite{ComE} For a pseudocode implementation including more detail, see algorithm 1 of the original ComE paper \cite{ComE}. \citeauthor{ComE_GH} published an implementation of the ComE algorithm on GitHub\footnote{at \url{https://github.com/andompesta/ComE}}.

\subsection{ComE Hyperparameters}
\label{sec:ComE_params}

ComE requires multiple hyperparameters to be set prior to computation. The following table lists the hyperparameters, a description for each, and the notation used in literature:

\begin{table}[htbp]
    \centering
    \caption{Hyperparameters used by \citeauthor{ComE_GH}'s implementation of ComE \cite{ComE_GH}.}
    \label{table:params_ComE}
    \begin{tabularx}{\linewidth}{ l | c | X }
        parameter            & notation & description                                                                     \\
        \hline
        \hline
        number\_walks        & $\gamma$ & number of random walks for each node                                            \\
        \hline
        walk\_length         & $\ell$   & length of each walk                                                             \\
        \hline
        representation\_size & $D$      & dimensionality of the embedding                                                 \\
        \hline
        num\_workers         & \empty   & number of threads                                                               \\
        \hline
        num\_iter            & \empty   & number of overall iterations                                                    \\
        \hline
        com\_n\_init         & \empty   & number of initializations to run on the communtiy embedding model (GMM or BGMM) \\
        \hline
        reg\_covar           & \empty   & regularization coefficient for ensuring positive covariance                     \\
        \hline
        batch\_size          & \empty   & size of the batch                                                               \\
        \hline
        window\_size         & $\zeta$  & windows size used to compute the context embedding                              \\
        \hline
        negative             & $m$      & number of negative samples                                                      \\
        \hline
        lr                   & \empty   & learning rate                                                                   \\
        \hline
        alpha                & $\alpha$ & Trade-off parameter for context embedding                                       \\
        \hline
        beta                 & $\beta$  & Trade-off parameter for community embedding                                     \\
        \hline
        down\_sampling       & \empty   & perform down sampling of common nodes                                           \\
        \hline
        communities          & $K$      & number of communities                                                           \\
    \end{tabularx}
\end{table}

\subsection{Non-Bayesian and Bayesian Gaussian mixture models}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{.4\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/ComE/GMM.png}
        \caption{Non-Bayesian}
    \end{subfigure}%
    \begin{subfigure}[b]{.5\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/ComE/BGMM.png}
        \caption{Bayesian}
    \end{subfigure}%
    \caption{Gaussian mixture models in plate notation\protect\footnotemark.}
    \label{fig:GMM_vs_BGMM_plates}
\end{figure}
\footnotetext{By Benwing - Created using LaTeX, TikZ, CC BY 3.0, \url{https://commons.wikimedia.org/w/index.php?curid=18934624}}

Figure \ref{fig:GMM_vs_BGMM_plates} compares a non-Bayesian and a Bayesian GMM side-by-side in plate notation: Squares indicate fixed parameters and circles indicate random variables.

The Bayesian Gaussian mixture model assumes all parameters of the Gaussian mixture model (number of communities $K$, means $\mu_k$, and covariances $\sigma_k$) are themselves generated by some distribution. This is represented in the plate diagrams in Figure \ref{fig:GMM_vs_BGMM_plates}.

A Bayesian Gaussian mixture model (BGMM) with variational inference (VI) offers a solution to the issue of introducing the number of communities $K$ as a fixed parameter, by taking advantage of Bayesian methods.

A BGMM with VI tends to utilize some components heavily while eliminating other components, if the provided dataset suggests so, by applying a tradeoff parameter. Therefore, the parameter $K$ represents a maximum of communities and not the final number of communities like for GMMs.

\subsection{Algorithm ComE BGMM+VI}
\label{sec:ComE_BGMM_alg}

The algorithm can stay mostly unchanged to the algorithm sketched out in Section \ref{sec:ComE_alg}. Only steps 3 and 4 need to be adjusted to handle community detection and embedding and node embeddings using a Bayesian Gaussian mixture model (BGMM) with variational inference (VI) instead of a non-Bayesian Gaussian mixture model (GMM) with expectation-maximization (EM).

A Bayesian Gaussian mixture model, just like a non-Bayesian Gaussian mixture model, produces mixed community membership $\Pi$, which means $\Psi$, and covariances $\Sigma$ for communities based on node embeddings $\Phi$. Therefore, the inputs and outputs of step 3 stay the same.

In their \citeyear{ComE+} paper \textit{Embedding Both Finite and Infinite Communities on Graphs}, \citeauthor{ComE+} present their version of ComE with an infinite Bayesian-approach to community detection and embedding. As of the time of writing of this paper, no source code for ComE+ was published. While ComE+ and ComE BGMM+VI both take a Bayesian approach to community embedding, ComE+ redefined the optimization function for higher-order proximity based on the Bayesian Gaussian mixture model, while ComE BGMM+VI simply uses a BGMM for community embedding and detection and leaves the node embedding step unchanged.

The implementation of ComE BGMM+VI\footnote{ComE BGMM+VI is published at \url{https://github.com/abegehr/ComE_BGMM} \cite{ComE_BGMM_GH}.} presented and used in this paper, utilizes sklearn's implementation of the Bayesian Gaussian mixture model and variational inference references at \textit{sklearn.mixture.BayesianGaussianMixture}.\cite{scikit-learn, sklearn_api}

\subsection{Hyperparameters BGMM}
\label{ComE_BGMM_params}

Switching from the original ComE's GMM to a Bayesian GMM, will require one additional parameter: the BGMM's weight concentration prior, or simply concentration.

The BGMM's concentration is a trade-off parameter that directly influences the number of components utilized for community embedding. A higher concentration parameter leads to more components active. A lower concentration parameter leads to fewer components active.

ComE with BGMM and VI has the same hyperparameters as ComE with GMM and EM, as presented in Section \ref{sec:ComE_params}, and in addition also the following:

\begin{table}[htbp]
    \centering
    \caption{Hyperparameter added to ComE for supporting a BGMM for community embedding \cite{ComE_BGMM_GH}.}
    \label{table:params_ComE_BGMM}
    \begin{tabularx}{\linewidth}{ l | c | X }
        parameter                    & notation & description                               \\
        \hline
        \hline
        weight\_concentration\_prior & $\Gamma$ & Bayesian GMM's weight concentration prior \\
    \end{tabularx}
\end{table}


\section{Visual Example}
\label{ch:come_visual}

In this section community embeddings based on a standard Gaussian mixture model with expectation-maximization (GMM+EM) and community embeddings based on a Bayesian Gaussian mixture model with variational inference (BGMM+VI) are compared visually. Two-dimensional visualizations are computed on a small graph-dataset using multiple values for the hyperparameter number of communities $K$ to show the advantage of BGMM+VI over GMM+EM under specific conditions.

\subsection{Karate Club Dataset}
\label{sec:karate_club}

\textit{Zachary’s Karate Club graph} is a popular graph dataset for visualizing graph algorithms, due to its small size \cite{KarateClubDS}.

The data was collected from a university karate club by \citeauthor{Zachary1977AnIF} as part of the \citeyear{Zachary1977AnIF} paper \textit{Zachary’s Karate Club graph}. The small Karate Club graph consists of 34 nodes, each representing one person as part of the karate club. The presence of an edge represents a social tie among two members of the group. The absence of an edge represents no social tie among the two members of the group. Edges are bidirectional. \textit{Zachary’s Karate Club graph} provided a binary version (ZACHE) and a weighted version (ZACHC) of the graph. We will consider the binary version of the karate club graph.

The following hyperparameters are used for computing both ComE with GMM and BGMM results:

\begin{table}[H]
    \centering
    \caption{Hyperparameters used for the Karate Club dataset. See Table \ref{table:params_ComE} for descriptions of hyperparameters.}
    \label{table:params_KCvisual}
    \begin{tabular}{ l | c | r }
        parameter                    & notation & value        \\
        \hline
        \hline
        number\_walks                & $\gamma$ & $10$         \\
        \hline
        walk\_length                 & $\ell$   & $80$         \\
        \hline
        representation\_size         & $D$      & $2$          \\
        \hline
        num\_workers                 & \empty   & $10$         \\
        \hline
        num\_iter                    & \empty   & $1$          \\
        \hline
        reg\_covar                   & \empty   & $0.00001$    \\
        \hline
        batch\_size                  & \empty   & $50$         \\
        \hline
        window\_size                 & $\zeta$  & $10$         \\
        \hline
        negative                     & $m$      & $5$          \\
        \hline
        lr                           & \empty   & $0.025$      \\
        \hline
        alpha                        & $\alpha$ & $0.1$        \\
        \hline
        beta                         & $\beta$  & $0.1$        \\
        \hline
        down\_sampling               & \empty   & $0.0$        \\
        \hline
        communities                  & $K$      & $[2,3,4,15]$ \\
        \hline
        weight\_concentration\_prior & $\Gamma$ & $10^{-5}$    \\
    \end{tabular}
\end{table}

For each $K \in [2,3,4,15]$ embeddings based on ComE with GMM and ComE with BGMM are computed. Each combination of a $K$ and choice of a mixture model for community detection and embedding, three charts are obtained:

\begin{enumerate}
    \item Graph: nodes positioned using the Fruchterman-Reingold force-directed (spring) algorithm and colored by community assignment.
    \item Embeddings: obtained node and community embeddings. Ellipses represent communities and the underlying Gaussian component. Points colored by the community assignment represent node embeddings.
    \item Weights: bar chart of weight for each community.
\end{enumerate}

\newcommand{\visresults}[2]{
    \begin{figure}[H]
        \centering
        \begin{subfigure}{.15\textwidth}
            \centering
            \includegraphics[width=1\linewidth]{images/results/#1/graph_#2.png}
            \caption{Graph}
        \end{subfigure}%
        %
        \begin{subfigure}{.15\textwidth}
            \centering
            \includegraphics[width=1\linewidth]{images/results/#1/embeddings_#2.png}
            \caption{Embeddings}
        \end{subfigure}%
        %
        \begin{subfigure}{.15\textwidth}
            \centering
            \includegraphics[width=1\linewidth]{images/results/#1/weights_#2.png}
            \caption{Weights}
        \end{subfigure}%
        %
        \caption{ComE with #1, $K=#2$}
        \label{fig:results#1#2}
    \end{figure}
}

\subsection{K=2}

\visresults{GMM}{2}
\visresults{BGMM}{2}

Figure \ref{fig:resultsGMM2} and Figure \ref{fig:resultsBGMM2}.
Two is the intended number of communities. Setting $K=2$ leads to similar results for both using a GMM and a BGMM for community detection and embedding. Although the labels are inverted, and the separation between both community assignments is slightly different, the general community assignment and structure are the same.

\subsection{K=3}

\visresults{GMM}{3}
\visresults{BGMM}{3}

Figure \ref{fig:resultsGMM3} and Figure \ref{fig:resultsBGMM3}.
Choosing $K=3$ leads to worse results when using a GMM in comparison to the BGMM. The GMM utilizes the three components, while the BGMM drops the unnecessary third component and utilizes only the two necessary components.

\subsection{K=4}

\visresults{GMM}{4}
\visresults{BGMM}{4}

Figure \ref{fig:resultsGMM4} and Figure \ref{fig:resultsBGMM4}.
With $K=4$, the quality of embeddings based on the GMM further declines. The GMM utilizes all four communities. The result obtained by the BGMM also declines for this example, but wins over the GMM. The BGMM utilizes three communities.

\subsection{K=15}

\visresults{GMM}{15}
\visresults{BGMM}{15}

Figure \ref{fig:resultsGMM15} and Figure \ref{fig:resultsBGMM15}.
Providing $K=15$ at initialization further underlines the potential advantage of BGMM over GMM. The GMM utilizes all 15 communities. It is clear that in this dataset of 34 nodes, no 15 communities are necessary for proper representation. The BGMM achieves a visually comparable quality to results at $K=2$ and $K=3$ by only utilizing 2 of the maximum 15 communities for representation.

\subsection{Summary}

As hypothesized, the community embedding algorithm ComE underperforms when the hyperparameter number of communities $K$ is unknown. A Bayesian approach with variational inference similar to ComE+ is proposed to use the tradeoff parameter weight concentration before dropping unused communities if $K$ is chosen too high.

The small Karate Club dataset, consisting only of 34 nodes, is used to test the hypothesis through a visual comparison experiment. The Karate Club graph is embedded into two-dimensional Euclidean space with ComE and ComE BGMM+VI for multiple values for the hyperparameter number of communities $K = [2, 3, 4, 15]$. The hypothesized advantage of BGMM over GMM for unknown numbers of communities $K$ when embedding with ComE becomes clear. Even with $K=15$, ComE BGMM+VI identifies the $2$ communities, while the non-Bayesian ComE utilizes all $15$ communities.















%%% Bibliography
\bibliographystyle{IEEEtranN}
\bibliography{literature}

\end{document}
